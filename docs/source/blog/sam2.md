:blogpost: true
:date: August 15, 2025
:author: Pille Wetterauer, Jyoti Bhogal
:location: London, UK
:category: Blog
:language: English
:image: 1

(target-sam2)=

# Exploring automatic ways of extracting a pose estimation skeleton for *C. elegans*
*Segmenting C. elegans using SAM-2 and extracting skeletons.*

```{image} /_static/blog_images/gsoc2025/gsoc-niu.png
:align: center
:width: 50%
```
<br>

Manually defining the pose skeletons in each video frame can be very tedious. 
Automating this process would make movement analysis a lot faster and easier. Therefore, 
our project at [OSW](https://neuroinformatics.dev/open-software-week/index.html) 
hackday aimed to explore ways of doing exactly that.

## Why worms?
When studying animal behaviour, _Caenorhabditis elegans_ is not the first model 
organism most people think of. However, they are used frequently as a model for 
drug discovery, developmental biology, genetics, and other areas. Changes in the 
behaviour are thereby an important readout, indicating the effectiveness of a drug, 
a developmental defect, or the contribution of a specific gene. The main advantage 
of using _C. elegans_ over mammalian models like mice is the simpleness. This is also 
true for the OSW hackday project described here: what could be easier to skeletonize 
than a worm!

## Segmenting worms with SAM-2
In order to extract pose skeletons, we first need to create segmentation masks, and 
of course this should be automated as well. There are lots of deep learning algorithms 
available for segmentation. Here, we try out 
[Segment Anything Model 2 (SAM-2)](https://github.com/facebookresearch/sam2). This model
can be applied both to images and videos and is designed to segment any object with 
minimal input from the user.

### Installation
Installing SAM-2 was one of the more challenging steps of this project. It requires:
* Python >= 3.10
* Pytorch and TorchVision
* SAM-2 package, that can be cloned from GitHub
* `ffmpeg` for video manipulation

Pytorch and SAM-2 are both installed using `pip`, so `pip` should be installed in the 
respective environment. Otherwise, it can come to version conflicts between packages 
from different environments, where `pip` is installed. 

`ffmpeg` is a commandline video manipulation tool, that is recommended in the example 
notebook for SAM-2 for extracting frames. It is installed outside the python 
environment. Of course, any other video manipulation tool can be used as well.

The installation instructions recommend using WSL on Windows. However, since I already 
had Python on my Windows system, I decided to ignore that. In the end it did work, but I 
kept getting some non-fatal error messages. The bigger problem, however, was the 
performance of my laptop. Even though it has a GPU, the prediction took several hours, 
so we decided to use Google Colab instead.

### Running the notebook on Google Colab
The SAM-2 repository contains sample notebooks for different use-cases, including video 
segmentation. The notebooks contain al link to Google Colab and code for setting up the 
Colab environment. You just need to choose a runtime with GPU (T4 for a free runtime), 
connect to it and mount Google Drive. The data has to be uploaded to Google Drive.

Google Colab is good for trying out things, however there are usage limits. The 
connection is determined after being inactive for a while and there is a limited number 
of sessions with GPU usage. Google does not publish these limits, apparently they vary. 
There are paid options for longer runtimes and more GPU types.

### SAM-2 workflow for video segmentation

SAM-2 repository provides an example notebook for segmenting videos, which is a good 
starting point for exploring this model. The video predictor is built with pretrained 
weights (need to be downloaded separately when using SAM-2 on a local machine!). The 
video data has to be saved as single frames, the example notebook uses JPEG files. These 
images are loaded into a variable called `Ã¬nference_state`. Then the user should provide 
prompts for the objects that should be segmented. There can be one or more objects and 
the prompts can be either point coordinates or boxes. Furthermore, the prompts have label,
showing whether the prompt is positive (i.e. marking the desired object) or negative (
marking the background). With this input the first masks for a given frame are predicted, 
as shown below. Here, point prompts are used, 2 positive and one negative for each worm.

```{image} /_static/blog_images/sam2/sam2-workflow.png
:align: center
:width: 100%
```
<br>
The resulting masks are visualized, so the user can decide whether to move on or add more 
prompts for a better result. Due to limited time, the masks here were not refined further.
The next step is to propagate the masks to whole video. Masks for each frame are predicted 
and each object is tracked throughout the video. This is the time-consuming step. A good 
innings to grab a cup of coffee (or a piece of pizza) and have a chat with fellow coders.

```{image} /_static/blog_images/sam2/sam2-propagation.tiff
:align: center
:width: 100%
```
<br>
As a result of the prediction you get masks for every frame in the video. The notebook 
displays some of them, so you can check the quality. All the predicted masks are stored 
is a variable called `video_segments`. Since the masks are numpy arrays, they are not 
serializable and cannot be stored in a JSON file. But you can use `pickle` to store them 
for later use.

The example notebook contains all these steps with different options and extra code cells 
for e.g. adding additional prompts, different kinds of prompts, only one object, etc. 
A more concise notebook, tailored for segmenting _C. elegans_, can be found 
[here](https://github.com/pwetterauer/WormNotebooks.git).

### Limitations
While the results look reasonably good, there are some limitations. First, even in this 
simple example the masks overlap not only the worm, but also some neighbouring pixels. 
Using more points to prompt the model might solve this issue. However, the point coordinates 
have to be entered manually. Finding out the coordinates and typing them into an array is 
not very convenient. There are plugins for FIJI (SAMJ-IJ, only works for images not for 
videos) or napari (e.g. microSAM for microscopic images, supports 2D, 3D and videos), 
that make this step of the workflow easier.  Second, on a more crowded plate where the worms touch each other, the 
model tends to lose track of single worms. 
